[rl_agents.trainer.evaluation:INFO] [BATCH=1/2]--------------------------------------- 
[rl_agents.trainer.evaluation:INFO] [BATCH=1/2][run_batched_episodes] #samples=0 
[rl_agents.trainer.evaluation:INFO] [BATCH=1/2]--------------------------------------- 
[rl_agents.agents.fitted_q.pytorch:INFO] Saved a replay memory of length 0 
[rl_agents.agents.fitted_q.pytorch:INFO] Saved a replay memory of length 0 
[rl_agents.trainer.evaluation:INFO] Saved FTQAgent model to /home/abhinavmodi/git_cloned/rl-agents/scripts/out/HighwayEnv/FTQAgent/baseline_20200503-224122_11300/checkpoint-0.tar 
[rl_agents.trainer.evaluation:INFO] Collecting 700 samples with 12 workers... 
[rl_agents.trainer.evaluation:INFO] Episode 0 score: 12.0 
[rl_agents.trainer.evaluation:INFO] Episode 1 score: 33.6 
[rl_agents.trainer.evaluation:INFO] Episode 3 score: 30.5 
[rl_agents.trainer.evaluation:INFO] Episode 4 score: 4.1 
[rl_agents.trainer.evaluation:INFO] Episode 5 score: 3.1 
[rl_agents.trainer.evaluation:INFO] Episode 7 score: 28.5 
[rl_agents.trainer.evaluation:INFO] Episode 9 score: 31.3 
[rl_agents.trainer.evaluation:INFO] Episode 10 score: 9.1 
[rl_agents.trainer.evaluation:INFO] Episode 12 score: 9.4 
[rl_agents.trainer.evaluation:INFO] Episode 13 score: 4.7 
[rl_agents.trainer.evaluation:INFO] Episode 14 score: 29.7 
[rl_agents.trainer.evaluation:INFO] Episode 16 score: 22.0 
[rl_agents.trainer.evaluation:INFO] Episode 17 score: 24.0 
[rl_agents.trainer.evaluation:INFO] Episode 19 score: 32.8 
[rl_agents.trainer.evaluation:INFO] Episode 21 score: 3.0 
[rl_agents.trainer.evaluation:INFO] Episode 22 score: 30.8 
[rl_agents.trainer.evaluation:INFO] Episode 24 score: 31.5 
[rl_agents.trainer.evaluation:INFO] Episode 26 score: 34.9 
[rl_agents.trainer.evaluation:INFO] Episode 28 score: 9.0 
[rl_agents.trainer.evaluation:INFO] Episode 29 score: 2.3 
[rl_agents.trainer.evaluation:INFO] Episode 30 score: 5.6 
[rl_agents.trainer.evaluation:INFO] Episode 31 score: 5.8 
[rl_agents.trainer.evaluation:INFO] Episode 33 score: 33.4 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 0 is 0.9930670261383057 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 1 is 0.6000523567199707 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 2 is 0.4510256052017212 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 3 is 0.3630104660987854 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 4 is 0.2578734755516052 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 5 is 0.30313557386398315 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 6 is 0.22181768715381622 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 7 is 0.2995031774044037 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 8 is 0.12382286041975021 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 9 is 0.21462857723236084 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 10 is 0.20809605717658997 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 11 is 0.2056806981563568 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 12 is 0.23415184020996094 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 13 is 0.2098933458328247 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 14 is 0.44922399520874023 
[rl_agents.trainer.evaluation:INFO] [BATCH=2/2]--------------------------------------- 
[rl_agents.trainer.evaluation:INFO] [BATCH=2/2][run_batched_episodes] #samples=700 
[rl_agents.trainer.evaluation:INFO] [BATCH=2/2]--------------------------------------- 
[rl_agents.agents.fitted_q.pytorch:INFO] Saved a replay memory of length 700 
[rl_agents.agents.fitted_q.pytorch:INFO] Saved a replay memory of length 700 
[rl_agents.trainer.evaluation:INFO] Saved FTQAgent model to /home/abhinavmodi/git_cloned/rl-agents/scripts/out/HighwayEnv/FTQAgent/baseline_20200503-224122_11300/checkpoint-1.tar 
[rl_agents.trainer.evaluation:INFO] Collecting 700 samples with 12 workers... 
[rl_agents.trainer.evaluation:INFO] Episode 35 score: 35.8 
[rl_agents.trainer.evaluation:INFO] Episode 37 score: 37.0 
[rl_agents.trainer.evaluation:INFO] Episode 39 score: 16.6 
[rl_agents.trainer.evaluation:INFO] Episode 40 score: 21.3 
[rl_agents.trainer.evaluation:INFO] Episode 42 score: 32.3 
[rl_agents.trainer.evaluation:INFO] Episode 44 score: 3.5 
[rl_agents.trainer.evaluation:INFO] Episode 45 score: 10.4 
[rl_agents.trainer.evaluation:INFO] Episode 46 score: 21.5 
[rl_agents.trainer.evaluation:INFO] Episode 48 score: 4.9 
[rl_agents.trainer.evaluation:INFO] Episode 49 score: 29.7 
[rl_agents.trainer.evaluation:INFO] Episode 50 score: 4.8 
[rl_agents.trainer.evaluation:INFO] Episode 52 score: 18.9 
[rl_agents.trainer.evaluation:INFO] Episode 53 score: 15.2 
[rl_agents.trainer.evaluation:INFO] Episode 55 score: 36.8 
[rl_agents.trainer.evaluation:INFO] Episode 56 score: 10.5 
[rl_agents.trainer.evaluation:INFO] Episode 58 score: 6.0 
[rl_agents.trainer.evaluation:INFO] Episode 59 score: 35.4 
[rl_agents.trainer.evaluation:INFO] Episode 61 score: 20.5 
[rl_agents.trainer.evaluation:INFO] Episode 62 score: 4.3 
[rl_agents.trainer.evaluation:INFO] Episode 63 score: 12.9 
[rl_agents.trainer.evaluation:INFO] Episode 65 score: 25.8 
[rl_agents.trainer.evaluation:INFO] Episode 66 score: 17.7 
[rl_agents.trainer.evaluation:INFO] Episode 68 score: 9.1 
[rl_agents.trainer.evaluation:INFO] Episode 69 score: 32.8 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 0 is 0.8146111965179443 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 1 is 0.5920023918151855 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 2 is 0.5730817914009094 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 3 is 0.4371073246002197 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 4 is 0.34042221307754517 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 5 is 0.5335078239440918 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 6 is 0.23067045211791992 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 7 is 0.321189284324646 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 8 is 0.3318707346916199 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 9 is 0.32023629546165466 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 10 is 0.26665881276130676 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 11 is 0.5401852130889893 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 12 is 0.35654735565185547 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 13 is 0.9609994888305664 
[rl_agents.agents.fitted_q.abstract:DEBUG] Bellman residual at iteration 14 is 0.5196710824966431 
[rl_agents.agents.fitted_q.pytorch:INFO] Saved a replay memory of length 1400 
[rl_agents.agents.fitted_q.pytorch:INFO] Saved a replay memory of length 1400 
[rl_agents.trainer.evaluation:INFO] Saved FTQAgent model to /home/abhinavmodi/git_cloned/rl-agents/scripts/out/HighwayEnv/FTQAgent/baseline_20200503-224122_11300/checkpoint-final.tar 
